import streamlit as st
import pandas as pd
import re
import time
import asyncio
import os
import traceback
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from concurrent.futures import ThreadPoolExecutor
# â”€â”€ Custom CSS Loader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def local_css(file_name):
Â Â Â Â try:
Â Â Â Â Â Â Â Â with open(file_name) as f:
Â Â Â Â Â Â Â Â Â Â Â Â st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
Â Â Â Â except FileNotFoundError:
Â Â Â Â Â Â Â Â pass
local_css("style.css")
# â”€â”€ Detect if on Streamlit Cloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
is_cloud = os.environ.get("STREAMLIT_SERVER_ENABLE_STATIC_SERVING", False)
# â”€â”€ Driver Factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def init_driver(headless_mode=True):
Â Â Â Â options = uc.ChromeOptions()
Â Â Â 
Â Â Â Â # Essential for cloud/headless
Â Â Â Â options.add_argument("--no-sandbox")
Â Â Â Â options.add_argument("--disable-dev-shm-usage")
Â Â Â Â options.add_argument("--disable-gpu")
Â Â Â Â options.add_argument("--window-size=1920,1080")
Â Â Â Â options.add_argument("--disable-blink-features=AutomationControlled")
Â Â Â 
Â Â Â Â # Additional stealth options
Â Â Â Â options.add_argument("--disable-web-security")
Â Â Â Â options.add_argument("--disable-features=IsolateOrigins,site-per-process")
Â Â Â Â options.add_argument("--allow-running-insecure-content")
Â Â Â 
Â Â Â Â # Realistic user agent
Â Â Â Â options.add_argument(
Â Â Â Â Â Â Â Â "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
Â Â Â Â Â Â Â Â "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36"
Â Â Â Â )
Â Â Â 
Â Â Â Â # Force headless on cloud
Â Â Â Â if is_cloud:
Â Â Â Â Â Â Â Â headless_mode = True
Â Â Â Â Â Â Â Â st.warning("Running in headless mode on Streamlit Cloud.")
Â Â Â 
Â Â Â Â if headless_mode:
Â Â Â Â Â Â Â Â options.add_argument("--headless=new")
Â Â Â Â else:
Â Â Â Â Â Â Â Â st.warning("Visible mode enabled for local debugging.")
Â Â Â 
Â Â Â Â try:
Â Â Â Â Â Â Â Â driver = uc.Chrome(
Â Â Â Â Â Â Â Â Â Â Â Â version_main=144,
Â Â Â Â Â Â Â Â Â Â Â Â options=options,
Â Â Â Â Â Â Â Â Â Â Â Â use_subprocess=True,
Â Â Â Â Â Â Â Â Â Â Â Â driver_executable_path=None
Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â # Set implicit wait
Â Â Â Â Â Â Â Â driver.implicitly_wait(10)
Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â if not headless_mode:
Â Â Â Â Â Â Â Â Â Â Â Â driver.maximize_window()
Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â st.success("âœ“ ChromeDriver initialized successfully")
Â Â Â Â Â Â Â Â return driver
Â Â Â 
Â Â Â Â except Exception as e:
Â Â Â Â Â Â Â Â st.error(f"ChromeDriver initialization failed: {str(e)}")
Â Â Â Â Â Â Â Â try:
Â Â Â Â Â Â Â Â Â Â Â Â st.info("Attempting fallback with auto-detection...")
Â Â Â Â Â Â Â Â Â Â Â Â driver = uc.Chrome(options=options, use_subprocess=True)
Â Â Â Â Â Â Â Â Â Â Â Â driver.implicitly_wait(10)
Â Â Â Â Â Â Â Â Â Â Â Â if not headless_mode:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â driver.maximize_window()
Â Â Â Â Â Â Â Â Â Â Â Â st.success("âœ“ ChromeDriver initialized (fallback mode)")
Â Â Â Â Â Â Â Â Â Â Â Â return driver
Â Â Â Â Â Â Â Â except Exception as e2:
Â Â Â Â Â Â Â Â Â Â Â Â st.error(f"Fallback failed: {str(e2)}")
Â Â Â Â Â Â Â Â Â Â Â Â st.stop()
def scrape_ahrefs_traffic(driver, url, max_wait):
    result = {
        "URL": url,
        "Website Name": "N/A",
        "Organic Traffic": "N/A",
        "Traffic Worth": "N/A",
        "Status": "Failed",
        "Debug": ""
    }

    try:
        # Use correct parameter name (input â†’ not always needed, but safer)
        full_url = f"https://ahrefs.com/traffic-checker/?input={url}&mode=subdomains"
        driver.get(full_url)
        time.sleep(2.5)

        # â”€â”€ Cloudflare handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        page_source_lower = driver.page_source.lower()
        if any(x in page_source_lower for x in ["cloudflare", "just a moment", "checking your browser", "cf-browser-verification"]):
            result["Debug"] = "Cloudflare detected â†’ waiting longer..."
            max_cf = min(max_wait, 45)
            start = time.time()
            cleared = False

            while time.time() - start < max_cf:
                cookies = driver.get_cookies()
                if any(c.get('name') == 'cf_clearance' for c in cookies):
                    cleared = True
                    result["Debug"] = "Cloudflare cleared (cf_clearance cookie found)"
                    break
                current = driver.page_source.lower()
                if all(x not in current for x in ["cloudflare", "just a moment", "checking your browser"]):
                    cleared = True
                    result["Debug"] = "Cloudflare screen disappeared"
                    break
                time.sleep(1.8)

            if not cleared:
                result["Debug"] += " | Cloudflare NOT cleared"
                result["Status"] = "Blocked by Cloudflare"
                return result

        # â”€â”€ Wait for results area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            WebDriverWait(driver, max_wait).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "[class*='Modal'], [class*='modal'], .ReactModalPortal"))
            )
            result["Debug"] += " | Modal / portal found"
        except TimeoutException:
            result["Debug"] += " | No modal detected after wait"
            # Some versions no longer use classic modal â€” try to find result cards anyway
            try:
                WebDriverWait(driver, 12).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "[class*='traffic'], [class*='visits'], [class*='value']"))
                )
                result["Debug"] += " | Found traffic-related elements anyway"
            except:
                result["Status"] = "No result container found"
                return result

        time.sleep(2.5)  # let React/Vue/etc. finish rendering

        # â”€â”€ Try to get website/domain name â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        candidates = driver.find_elements(By.CSS_SELECTOR,
            "h1, h2, h3, .ReactModalPortal p, [class*='domain'], [class*='title'], [class*='header'] strong"
        )
        for el in candidates:
            txt = el.text.strip()
            if txt and len(txt) > 3 and "." in txt and not txt.startswith("$") and not any(c.isdigit() for c in txt[:4]):
                result["Website Name"] = txt
                break

        if result["Website Name"] == "N/A":
            # Fallback: look for first big text in modal
            try:
                result["Website Name"] = driver.find_element(
                    By.CSS_SELECTOR, ".ReactModalPortal [class*='title'], .ReactModalPortal h2, .ReactModalPortal p"
                ).text.strip()
            except:
                pass

        # â”€â”€ Organic Traffic (look for visits / traffic number with K/M/B) â”€â”€â”€â”€â”€
        traffic_candidates = driver.find_elements(By.XPATH,
            "//*[contains(text(),'K') or contains(text(),'M') or contains(text(),'B') or contains(text(),'visits') or contains(text(),'traffic')]"
        )

        for el in traffic_candidates:
            txt = el.text.strip()
            if any(suffix in txt.lower() for suffix in ["k", "m", "b", " visits", " traffic"]):
                # Usually the first big number near "organic" or standalone is it
                if "organic" in el.get_attribute("outerHTML").lower() or "search" in el.get_attribute("outerHTML").lower():
                    result["Organic Traffic"] = txt
                    break
                elif result["Organic Traffic"] == "N/A" and any(c in txt for c in "KM"):
                    result["Organic Traffic"] = txt  # best guess

        # â”€â”€ Traffic Worth ($) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        worth_candidates = driver.find_elements(By.XPATH,
            "//*[contains(text(),'$')]"
        )
        for el in worth_candidates:
            txt = el.text.strip()
            if txt.startswith("$") and any(c.isdigit() for c in txt):
                # Usually the biggest / first one near "worth" or "value"
                parent_html = el.find_element(By.XPATH, "..").get_attribute("outerHTML").lower()
                if any(w in parent_html for w in ["worth", "value", "traffic value", "usd"]):
                    result["Traffic Worth"] = txt
                    break
                elif result["Traffic Worth"] == "N/A":
                    result["Traffic Worth"] = txt  # fallback

        # â”€â”€ Final decision â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        has_data = any(x != "N/A" for x in [result["Website Name"], result["Organic Traffic"], result["Traffic Worth"]])

        if has_data:
            result["Status"] = "Success"
            result["Debug"] += " | Data extracted"
        else:
            result["Status"] = "No data found"
            result["Debug"] += " | Found container but could not match numbers/names"

    except Exception as e:
        result["Status"] = "Error"
        result["Debug"] = f"Exception: {str(e)[:180]}â€¦"

    return result
# â”€â”€ Batch processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def process_urls(urls, max_wait, headless, progress_callback=None):
Â Â Â Â driver = init_driver(headless_mode=headless)
Â Â Â Â loop = asyncio.get_event_loop()
Â Â Â Â executor = ThreadPoolExecutor(max_workers=1)
Â Â Â 
Â Â Â Â results = []
Â Â Â Â total = len(urls)
Â Â Â Â start = time.time()
Â Â Â Â for i, url in enumerate(urls):
Â Â Â Â Â Â Â Â row = await loop.run_in_executor(executor, scrape_ahrefs_traffic, driver, url, max_wait)
Â Â Â Â Â Â Â Â results.append(row)
Â Â Â Â Â Â Â Â elapsed = time.time() - start
Â Â Â Â Â Â Â Â eta = (elapsed / (i+1)) * (total - i - 1) if i < total-1 else 0
Â Â Â Â Â Â Â Â success = sum(1 for r in results if r["Status"] == "Success")
Â Â Â Â Â Â Â Â if progress_callback:
Â Â Â Â Â Â Â Â Â Â Â Â progress_callback(i+1, total, success, round(eta/60, 1), results)
Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â # Small delay between requests to appear more human
Â Â Â Â Â Â Â Â time.sleep(2)
Â Â Â Â driver.quit()
Â Â Â Â return results
# â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Ahrefs Traffic Bulk Checker", layout="centered")
st.title("ğŸ” Ahrefs Traffic Checker â€“ Bulk Extraction")
st.caption("2026 Cloud Version â€¢ Enhanced Cloudflare Detection â€¢ Exact XPath Targeting")
# Controls
col1, col2, col3 = st.columns([3, 2, 2])
with col1:
Â Â Â Â uploaded_file = st.file_uploader("ğŸ“ Upload CSV/XLSX", type=["csv", "xlsx"])
with col2:
Â Â Â Â max_wait = st.number_input("â±ï¸ Max wait per URL (sec)", 30, 180, 70, 5)
with col3:
Â Â Â Â headless = st.checkbox("ğŸ¤– Run Headless", value=True,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â help="Headless mode required on cloud deployment")
if uploaded_file is not None:
Â Â Â Â try:
Â Â Â Â Â Â Â Â df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
Â Â Â Â Â Â Â Â url_col = st.selectbox("Select URL column", df.columns)
Â Â Â Â Â Â Â Â urls = df[url_col].dropna().unique().tolist()
Â Â Â Â Â Â Â Â st.markdown(f"**ğŸ“Š {len(urls)} unique URLs found**")
Â Â Â Â Â Â Â Â if st.button("â–¶ï¸ Start Processing", type="primary"):
Â Â Â Â Â Â Â Â Â Â Â Â spinner = st.empty()
Â Â Â Â Â Â Â Â Â Â Â Â spinner.markdown(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â """
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <div style="display:flex; align-items:center; gap:12px;">
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <div class="loader"></div>
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <span>Initializing scraper...</span>
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â </div>
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <style>.loader {border:5px solid #f3f3f3;border-top:5px solid #3498db;border-radius:50%;width:28px;height:28px;animation:spin 1s linear infinite;} @keyframes spin {0% {transform:rotate(0deg);} 100% {transform:rotate(360deg);}}</style>
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â """, unsafe_allow_html=True
Â Â Â Â Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â Â Â Â Â progress = st.progress(0)
Â Â Â Â Â Â Â Â Â Â Â Â status = st.empty()
Â Â Â Â Â Â Â Â Â Â Â Â table = st.empty()
Â Â Â Â Â Â Â Â Â Â Â Â def update_ui(current, total, success_count, eta_min, current_results):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â progress.progress(current / total)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â status.markdown(f"**Progress:** {current}/{total} â€¢ **âœ“ Success:** {success_count} â€¢ **â³ ETA:** ~{eta_min} min")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Show results table with only required columns
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â df_results = pd.DataFrame(current_results)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Reorder columns for better display
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â column_order = ["URL", "Website Name", "Organic Traffic", "Traffic Worth", "Status", "Debug"]
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â df_results = df_results[column_order]
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â table.dataframe(df_results, use_container_width=True)
Â Â Â Â Â Â Â Â Â Â Â Â results = asyncio.run(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â process_urls(urls, max_wait, headless=headless, progress_callback=update_ui)
Â Â Â Â Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â Â Â Â Â spinner.empty()
Â Â Â Â Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â # Show summary
Â Â Â Â Â Â Â Â Â Â Â Â success_count = sum(1 for r in results if r["Status"] == "Success")
Â Â Â Â Â Â Â Â Â Â Â Â blocked_count = sum(1 for r in results if "Cloudflare" in r["Status"] or "blocked" in r["Debug"].lower())
Â Â Â Â Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â if success_count > 0:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.success(f"âœ… Processing finished! {success_count}/{len(results)} successful")
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.error(f"âš ï¸ No successful scrapes. {blocked_count} blocked by Cloudflare. Check Debug column.")
Â Â Â Â Â Â Â Â Â Â Â Â if results:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â final_df = pd.DataFrame(results)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Reorder columns for export
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â column_order = ["URL", "Website Name", "Organic Traffic", "Traffic Worth", "Status", "Debug"]
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â final_df = final_df[column_order]
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â csv = final_df.to_csv(index=False).encode('utf-8')
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.download_button(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "â¬‡ï¸ Download Results CSV",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â data=csv,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â file_name=f"ahrefs_traffic_{time.strftime('%Y%m%d_%H%M')}.csv",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â mime="text/csv"
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Show status breakdown
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.markdown("### ğŸ“ˆ Status Breakdown")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â status_counts = final_df['Status'].value_counts()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.dataframe(status_counts, use_container_width=True)
Â Â Â Â except Exception as e:
Â Â Â Â Â Â Â Â st.error(f"âŒ Error reading file: {str(e)}")
Â Â Â Â Â Â Â Â st.error(traceback.format_exc())
st.markdown("---")
st.markdown("""
### â„¹ï¸ Troubleshooting Guide

**All "N/A" results**: Cloudflare is blocking requests. Try:
Â Â - Increase wait time to 90+ seconds
Â Â - Run during off-peak hours
Â Â - Consider using residential proxies (not included in this version)
**"Modal not found"**: Page didn't load properly, increase timeout
**"Blocked by Cloudflare"**: Strong anti-bot protection detected
Check the **Debug** column for specific error details

### ğŸ“ XPath Selectors Used:

**Website Name**: /html/body/div[6]/div/div/div/div/div[1]/div/div[1]/p
**Organic Traffic**: /html/body/div[6]/div/div/div/div/div[2]/div[1]/div[1]/div/div/div[1]/div[1]/div[2]/div/div/div/span
**Traffic Worth**: /html/body/div[6]/div/div/div/div/div[2]/div[1]/div[1]/div/div/div[1]/div[2]/div[2]/div/div/div/span
""")
